<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Probabilistic Deep Learning for Weak Lensing: from Mass-Mapping to Cosmological Parameter Inference</title>

	<meta name="description" content="LAM Seminar, Zoom, March 25th 2022">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>Probabilistic Deep Learning for Weak Lensing</h1>
						<h2>from Mass-Mapping to Cosmological Parameter Inference</h2>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>François Lanusse</h2>
								<br>
								<img src="assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="assets/logo_cnrs.png" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a href="https://eiffl.github.io/LAM2022">eiffl.github.io/LAM2022</a> </div>
				</div>
			</section>

		<section data-background-image="assets/WMAP_timeline_large.jpg">
			<h3 class='slide-title' style="position:absolute;top:0;"> the $\Lambda$CDM view of the Universe </h3>
			<br> <br>
			<div class="container">
				<div class="col" style="flex: 0 0 40em;">

				</div>
				<div class="col">

					<img class="plain" data-src="assets/Euclid.png" style="width: 300px" />

					<img class="plain" data-src="assets/wfirstlogo.png" style="width: 300px" />

					<img class="plain" data-src="assets/vrro.png" style="width: 300px" />
				</div>
			</div>
			<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
		</section>

			<section>
				<section data-background-video="assets/animation-day-to-night.mov" data-background-video-muted>
					<h3 class='slide-title'>the Rubin Observatory Legacy Survey of Space and Time</h3>
					<div class="container">
						<div class="col">
							<ul>
								<li class="fragment fade-up"> 1000 images each night, 15 TB/night for 10 years</li>
								<br>
								<li class="fragment fade-up"> 18,000 square degrees, observed once every few days</li>
								<br>
								<li class="fragment fade-up"> Tens of billions of objects, each one observed $\sim1000$ times</li>
							</ul>
						</div>

						<div class="col">
							<video data-autoplay class="fragment fade-up" data-fragment-index="1" data-src="assets/obsim.mp4" type="video/mp4" />
						</div>
					</div>
				</section>

				<section data-transition="fade-in fade-out" data-background="assets/gal_sdss.png" data-vertical-align-top>
					<p>Previous generation survey: SDSS</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="assets/gal_des.png" data-vertical-align-top>
					<p>Current generation survey: DES</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="assets/gal_hsc.png" data-vertical-align-top>
					<p>LSST precursor survey: HSC</p>

					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
			</section>

	<section>
				<h3 class='slide-title'>The challenges of modern surveys</h3>
				<div style="float:left;">
					$\Longrightarrow$ Modern surveys will provide <b>large volumes</b> of <b>high quality</b> data
				</div>
				<br>
				<div class="container">
					<div class="col">

						<div class="block fragment">
							<div class="block-title">
								A Blessing
							</div>
							<div class="block-content">
								<ul>
									<li>Unprecedented statistical power
									</li>
									<br>
									<li>Great potential for new discoveries
									</li>
								</ul>
							</div>
						</div>

						<br>
						<div class="block fragment">
							<div class="block-title">
								A Curse
							</div>
							<div class="block-content">
								<ul>
									<li> Existing methods are reaching their limits at every
										step of the science analysis
									</li>
									<br>
									<li>Control of systematics is <b>paramount</b>
									</li>
								</ul>
							</div>
						</div>

					</div>
					<div class="col">
						LSST forecast on dark energy parameters
						<img class="plain" data-src="assets/LSST_forecast_Y10.png" style="height :400px;" />
					</div>
				</div>
				<div class="fragment">
					$\Longrightarrow$ Dire need for <b class="alert">novel analysis techniques</b> to fully realize the potential of modern surveys.
				</div>
			</section>

			<section>
			<section class="inverted" data-background="#000">
				<h2> Can AI solve all of our problems?</h2>
			</section>

			<section>
				<h3 class="slide-title">The AI bubble</h3>

				<div class="container">
					<canvas data-chart="bar">
						<!--
							{
							 "data": {
								"labels": ["2012", "2013", "2014","2015", "2016" ,"2017", "2018", "2019", "2020", "2021"],
								"datasets": [
								 {
									"data":[ 21, 15, 17, 22, 31, 71, 113, 231, 322, 476],
									"label":"Deep Learning || CNN || Neural Network ","backgroundColor":"#A63446"
								}
								]
							 },
							 "options": { "responsive": "true",
						"scales": {
								"yAxes": [{
										"type": "linear"
								}]
						}
							}
							}
							-->
					</canvas><br>
				</div>
				<div><b>astro-ph</b> abstracts mentioning <b>Deep Learning</b>, <b>CNN</b>, or <b>Neural Networks</b></div>
			</section>

		</section>

			<section>
        <section data-background="assets/gal_hsc.png">
					<div class="fragment">
						<div style="float:right; font-size: 20px">Branched GAN model for deblending <a href="https://arxiv.org/abs/1810.10098">(Reiman & Göhre, 2018)</a></div>

						<img class="plain" data-src="assets/Reiman2018_1.png" />
					</div>

					<div class="block fragment">
						<div class="block-title">
							The issue with using deep learning as a <i>black-box</i>
						</div>
						<div class="block-content">
							<ul>
								<li> No explicit control of noise, PSF, number of sources.
									<ul>
										<li> Model would have to be retrained for all observing configurations
										</li>
									</ul>
								</li>
								<br>
								<li class="fragment"> No guarantees on the network output (e.g. flux preservation, artifacts)
								</li>
								<br>
								<li class="fragment"> No proper uncertainty quantification.
								</li>
							</ul>
						</div>
					</div>
				</section>

				<section>
					<img class="plain" data-src="assets/Reiman2018_3.png" />
				</section>
			</section>
<!--

      			<section>
      				<section>
      					<h3 class="slide-title" style="position:absolute;top:0;">A Motivating Example: Image Deconvolution</h3>
      					<br>
      					<br>

      					$ y = P \ast x + n $

      					<div class="container">

      						<div class="col">
      							<p> <b class="alert"> Observed $y$</b></p>
      							<img class="plain" data-src="assets/cosmos_gal_ground.png" style="width: 250px" />
      							<br>Ground-Based Telescope
      						</div>

      						<div class="col fragment fade-up" data-fragment-index='0'>
      							<p> <b class="alert">$f_\theta$</b> </p>
      							<img class="plain " data-src="assets/generic_network_inv.png" style="height: 250px; width:500px" />
      							<br>some deep Convolutional Neural Network
      						</div>

      						<div class="col">
      							<p><b class="alert"> Unknown $x$</b> </p>
      							<img class="plain" data-src="assets/cosmos_gal.png" style="width: 250px" />
      							<br>Hubble Space Telescope
      						</div>
      					</div>
      					<br>
      					<br>
      					<ul>
      						<li class="fragment fade-up" data-fragment-index='0'> A standard approach would be to train a neural network $f_\theta$ to <b class="alert">estimate $x$ given $y$</b>.
      						</li>
      					</ul>
      				</section>

      				<section>
      					<ul>
      						<li> <i>Step I</i>: Assemble from <b>data</b> or <b>simulations</b> a <b class="alert">training set</b> of images
      							$$\mathcal{D} = \{(x_0, y_0),
      							(x_1, y_1), \ldots, (x_N, y_N) \}$$
      							$\Longrightarrow$ the dataset contains <b class="alert">hardcoded assumptions</b> about PSF $P$
      							noise $n$, and galaxy morphology $x$.
      						</li>
      						<li class="fragment fade-up"> Step II: Train the neural network $f_\theta$ under a <b class="alert">regression loss</b> of the type:
      							$$ \mathcal{L} = \sum_{i=0}^N \parallel x_i - f_\theta(y_i)\parallel^2 $$
      						</li>
      					</ul>
      					<div class="container fragment">
      						<div class="col">
      							<img class="plain" data-src="assets/cosmos_gal_ground.png" style="width: 250px" />
      							<p>$$ y $$</p>
      						</div>

      						<div class="col">
      							<img class="plain " data-src="assets/generic_network_inv.png" style="height: 250px; width:500px" />
      							<p>$$f_\theta$$</p>
      						</div>

      						<div class="col">
      							<img class="plain" data-src="assets/rec_median.png" style="width: 250px" />
      							$$f_\theta(y)$$
      						</div>

      						<div class="col fragment fade-up" style="float:center;">
      							<img class="plain" data-src="assets/cosmos_gal.png" style="width: 250px" />
      							<br>
      							<p>$$ \mbox{True } x $$</p>
      						</div>
      					</div>
      					<div class="fragment">$\Longrightarrow$Why is the network output different from the truth? If it's not the truth, then <b>what is $f_\theta(y)$?</b></div>
      				</section>

      				<section>
      					<p>Let's try to understand the neural network output by looking at the <b class="alert">loss function</b></p>
      					$$ \mathcal{L} = \sum_{(x_i, y_i) \in \mathcal{D}} \parallel x_i - f_\theta(y_i)\parallel^2 \quad \simeq \quad \int \parallel x - f_\theta(y) \parallel^2 \ p(x,y) \ dx dy $$

      					<div class="fragment" data-fragment-index="1">$$\Longrightarrow \int \left[ \int \parallel x - f_\theta(y) \parallel^2 \ p(x|y) \ dx \right] p(y) dy $$ </div>

      					<div class="block fragment" data-fragment-index="2">
      						<div class="block-content">
      							$\mathcal{L}$ minimized when $f_{\theta^\star}(y) = \int x \ p(x|y) \ dx $, i.e.
      							when <b class="alert">$f_{\theta^\star}(x)$ is predicting the mean of $p(x|y)$</b>.
      						</div>
      					</div>
      					<div class="container">
      						<div class="col">
      							<div style="position:relative; width:500px; height:500px; margin:0 auto;">
      								<img class="fragment current-visible plain" data-src="assets/nn_l2.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
      								<img class="fragment current-visible plain" data-src="assets/nn_l2_mean.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
      								<img class="fragment current-visible plain" data-src="assets/nn_l1.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="5" />
      								<img class="fragment plain" data-src="assets/nn_l1_median.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="6" />
      							</div>
      						</div>
      						<div class="col">
      							<ul>
      								<li class="fragment" data-fragment-index="3"> Using an <b class="alert">$\ell_2$ loss learns the mean</b> of the $p(x | y)$
      								</li>
      								<br>
      								<li class="fragment" data-fragment-index="5"> Using an <b class="alert">$\ell_1$ loss learns the median</b> of $p(x|y)$
      								</li>
      								<br>
      								<li class="fragment" data-fragment-index="7"> In general, training a neural network for regression doesn't
      									achieve de mode of the distribution.<br>
      									<br>
      									<div style='vertical-align:middle; display:inline;'>Check this <a href="https://medium.com/cosmostat/regression-in-the-presence-of-uncertainties-with-tensorflow-probability-1b7449f1083b" target="blank_">blogpost</a> and this
      										notebook to learn how to do that: <a href=" https://colab.research.google.com/drive/1yi_BY09LCS8qHCfJqvCIftKuW6jNe-t1" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"
      												class="plain" style="height:25px;vertical-align:middle; display:inline;" /></a></div>
      								</li>
      							</ul>
      						</div>
      					</div>
      				</section>
      			</section>

					<section>
      			<section>
      				<h3 class="slide-title">A Bayesian understanding of a regression network</h3>
      				<div class="container">
      					<div class="col">
      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
      							<img class="plain" data-src="assets/cosmos_gal_ground.png" style="position:absolute;top:0;left:0;width:200px;" />
      						</div>
      						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
      							<div class='col ' style="position:absolute;top:0;left:0;width:200px;"> Data $y$</div>
      						</div>
      						<br>
      					</div>

      					<div class="col" data-fragment-index='0'>
      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
      							<div><video data-autoplay data-loop data-src="assets/rec_samples.mp4" type="video/mp4" style="height: 200px;" />
      							</div>
      						</div>
      						<div>Posterior samples</div>
      					</div>

      					<div class="col">
      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
      							<img class="plain " data-src="assets/rec_median.png" style="position:absolute;top:0;left:0;width:200px;" />
      						</div>
      						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
      							<div class='col' style="position:absolute;top:0;left:0;width:200px;">Posterior mean</div>
      						</div>
      					</div>

      					<div class="col">
      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
      							<img class="plain " data-src="assets/cosmos_gal.png" style="position:absolute;top:0;left:0;width:200px;" />
      						</div>

      						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
      							<div class='col' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> True $x$</div>
      						</div>
      					</div>
      				</div>
      				<br>
      				<ul>
      					<li> The distribution $p(x|y)$ can be understood as a <b>Bayesian posterior distribution</b>:
      						$$ p(x | y) \propto \underbrace{p(y | x)}_{\mbox{likelihood}} \quad \underbrace{p(x)}_{\mbox{prior}} $$
      					</li>
      					<br>
      					<li class="fragment"> Both priors and likelihoods are <b>learned implicitly</b> by the neural network from the training set.
      						<br>$\Longrightarrow$ <b class="alert">priors AND likelihoods are hardcoded</b> in the training set.
      					</li>
                <br>
                <li class="fragment"> The network only returns a point summary of this posterior distribution (<b>no uncertainty quantification</b>).
                </li>
      				</ul>
      			</section>

						<section>
							<h3 class="slide-title">welcome to...</h3>
									<img data-src="assets/danger_zone.png" style="height: 600px;" />
						</section>
					</section> -->

            <section>
              <h3 class="slide-title">Focus of this talk</h3>
              <div class=container>
                <div class="col">
                  <div class="fig-container" data-file="venn.html" data-style="height: 600px;"></div>
                </div>

                <div class="col">
                  <ul>
                    <li class="fragment">Deep Learning for astronomical data reduction</li>
                    <br>
                    <li class="fragment">Bayesian Neural Networks</li>
                    <br>
                    <li class="fragment">Physical Bayesian inference</li>
                  </ul>
                  <br>
                  <br>
                  <div class="block fragment">
                    <div class="block-title">
                      This talk
                    </div>
                    <div class="block-content">
                      Generic approach to <b>uncertainty quantification</b> and <b>interpretability</b>:
                      <br>
                      <ul>
                        <li>(Differentiable) Physical Forward Models</li>
                        <li>Deep Generative Models</li>
                        <li>Bayesian Inference</li>
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            </section>


						<section>
							<h2>Probabilistic Weak Lensing Mass Mapping by<br> Neural Score Matching</h2>
							<a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;" /></a>
							<a href="https://github.com/CosmoStat/jax-lensing"><img src="https://badgen.net/badge/icon/github?icon=github&label" class="plain" style="height:25px;" /></a>
							<hr>
							<div class="container">
								<div class="col">
									<div align="left" style="margin-left: 20px;">
										<h3>Work in collaboration with: <br>
											Benjamin Remy, Niall Jeffrey
										</h3>
										<img data-src="assets/benjamin.png" style='width:200px; height:200px;'></img>
										<img data-src="assets/niall.jpg" style='width:200px; height:200px;object-fit: cover;'></img>

										<br>

										$\Longrightarrow$ <b class="alert">Learn complex priors</b> by Neural Score Estimation and <b class="alert">sample from posterior</b> with gradient-based MCMC.
									</div>
								</div>
								<div class="col">
									<img class="plain" data-src="assets/cropped.gif" style="width:450px;" />
								</div>
							</div>
							<br>
						</section>


									<section>
										<section data-background-image="assets/gravitational-lensing-diagram.jpg">
											<h3 class="slide-title"> Gravitational lensing</h3>
											<div class="fragment fade-up">
												<img class="plain" data-src="assets/great.jpg" />

												<div class="block ">
													<div class="block-title">
														Galaxy shapes as estimators for gravitational shear
													</div>
													<div class="block-content">
														$$ e = \gamma + e_i \qquad \mbox{ with } \qquad e_i \sim \mathcal{N}(0, I)$$
														<ul>
															<li> We are trying the measure the <b class="alert"> ellipticity $e$</b> of
																galaxies as an estimator for the <b class="alert">gravitational shear $\gamma$ </b>
															</li>
														</ul>
													</div>
												</div>
											</div>
										</section>

										<section>
											<h3 class="slide-title">Weak Lensing Mass-Mapping as an Inverse Problem</h3>
											<div class="container">
												<div class="col">
													Shear <b class="alert">$\gamma$</b><br>
													<img data-src="assets/shear_cat1.png" style="width:450px;"></img>
												</div>

												<div class="col fragment fade-up">
													Convergence <b class="alert">$\kappa$</b><br>
													<img data-src="assets/kappa.png" style="width:450px;"></img>
												</div>
											</div>

											<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
												<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
													$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
												</div>
												<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
													$$\boxed{\gamma = \mathbf{P} \kappa}$$
												</div>
											</div>
										</section>

										<section>
											<h3 class="slide-title"> Illustration on the Dark Energy Survey (DES) Y3</h3>
											<div style="float:right; font-size: 20px">Jeffrey, et al. (2021)
											</div><br>
											<img data-src="assets/DESY3map.png" style="height:600px;"></img>
										</section>
									</section>

									<section>
										<h3 class="slide-title">Linear inverse problems</h3>

										$\boxed{y = \mathbf{A}x + n}$
										<br>
										<br>
										$\mathbf{A}$ is known and encodes our physical understanding of the problem.
										<span class="fragment"><br>$\Longrightarrow$ When non-invertible or ill-conditioned, the inverse problem is ill-posed <b class="alert">with no unique solution $x$</b></span>
										<div class="container fragment fade-up">
											<div class="col">
												<img data-src="assets/pluto_smooth.png" class="plain"></img>
												Deconvolution
											</div>
											<div class="col">
												<img data-src="assets/pluto_missing.png" class="plain"></img>
												Inpainting
											</div>
											<div class="col">
												<img data-src="assets/plutoNoise.png" class="plain"></img>
												Denoising
											</div>
										</div>
									</section>

									<section>
										<section data-vertical-align-top>
											<h3 class="slide-title">What Would a Bayesian Do?</h3>
											$\boxed{y = \mathbf{A}x + n}$
											<br>
											<br>
											The Bayesian view of the problem:
											<br>
											$$ p(x | y) \propto p(y | x) \ p(x) $$
											<ul>
												<li class="fragment fade-up">$p(y | x)$ is the data <b>likelihood</b>, which <b class="alert">contains the physics</b><br>
												</li>
												<br>
												<li class="fragment fade-up">$p(x)$ is the <b>prior</b> knowledge on the solution.</li>
											</ul>
											<br>
											<br>
											<div class="fragment fade-up">
												<ul>With these concepts in hand we can:
													<br>
													<li class="fragment">Estimate for instance the <b>Maximum A Posteriori</b> solution:
														<br>
														$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
													</li>
													<li class="fragment">Estimate from the <b>full posterior p(x|y)</b> with MCMC or Variational Inference methods.
													</li>
												</ul>
											</div>
											<br>
											<div class="fragment fade-up">
												<h3>How do you choose the prior ?</h3>
											</div>
										</section>

										<section>
											<h3 class="slide-title"> Classical examples of signal priors </h3>
											<div class="container">
												<div class="col">
													Sparse
													<img data-src="assets/wavelet.png" height="400" class="plain"></img><br>
													$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
												</div>
												<div class="col">
													Gaussian
													<img data-src="assets/zknj8.jpg" height="400" class="plain"></img>
													$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
												</div>
												<div class="col">
													Total Variation
													<img data-src="assets/shepp-Logan.ppm" class="plain"></img>
													$$ \log p(x) = \parallel \nabla x \parallel_1 $$

												</div>
											</div>
										</section>

										<section data-background-image="assets/convergence.png">
											<h2>But what about this?</h2>
											<br>
											<br>
											<br>
											<div class="fragment"> $\Longrightarrow$ <b>Implicit prior</b> $p(x)$ in the form of <b>cosmological simulations</b>.</div>

											<!-- <div class="container">
															<div class="col">
																<img data-src="assets/gal_hsc.png" style="object-fit: cover; width:500px;height:500px;">
																<div class="fragment"> $\Longrightarrow$ Prior in the form of existing data.</div>
															</div>
															<div class="col fragment fade-up">
																<img data-src="assets/convergence.png" style="object-fit: cover; width:500px;height:500px;">
																<div class="fragment"> $\Longrightarrow$ Prior in the form of numerical simulations.</div>
															</div>
														</div> -->
										</section>
									</section>

									<section class="inverted" data-background="#000">
										<h2> Can we use Deep Learning to embed simulation-driven priors within a <b>physical Bayesian model</b>?</h2>
									</section>




															      			<section>
															      				<section>
															      					<h3 class="slide-title"> What is generative modeling?</h3>
															      					<br>
															      					<ul>
															      						<li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
															      							from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
															      						</li>
															      						<br>
															      						<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
															      							that tries to be close to $\mathbb{P}$.
															      						</li>
															      					</ul>

															      					<br>
															      					<div class="container">
															      						<div class="col fragment fade-up">
															      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
															      							<br>
															      							True $\mathbb{P}$
															      						</div>

															      						<div class="col  fragment fade-up">
															      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
															      							<br>
															      							Samples $x_i \sim \mathbb{P}$
															      						</div>

															      						<div class="col  fragment fade-up">
															      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
															      							<br>
															      							Model $\mathbb{P}_\theta$
															      						</div>
															      					</div>
															      					<br>
															      					<br>
															      					<ul>
															      						<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
															      						</li>
															      					</ul>

															      				</section>

															      				<section>
															      					<h3 class="slide-title">Why isn't it easy?</h3>
															      					<br>
															      					<ul>
															      						<li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
															      						</li>
															      					</ul>
															      					<div class="container">
															      						<div class="col fragment fade-up">
															      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png" class="plain"></img>
															      						</div>

															      						<div class="col fragment fade-up">
															      							<img style="height:350px;" data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg" class="plain"></img>
															      							<br>Distance between pairs of points drawn from a Gaussian distribution.
															      						</div>
															      					</div>

															      					<br>
															      					<ul>
															      						<li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
															      						</li>
															      					</ul>
															      				</section>
															      			</section>

															      			<section>
															      				<h3 class="slide-title"> The evolution of generative models </h3>

															      				<br> <br> <br>
															      				<div class='container'>
															      					<div class='col'>
															      						<div style="position:relative; width:500px; height:500px; margin:0 auto;">
															      							<img class="fragment current-visible plain" data-src="assets/DBN.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
															      							<img class="fragment current-visible plain" data-src="assets/vae_faces.jpg" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
															      							<img class="fragment current-visible plain" data-src="assets/gan-samples-1.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
															      							<img class="fragment plain" data-src="assets/karras2017.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
															      						</div>
															      					</div>

															      					<div class='col'>
															      						<ul>
															      							<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006) </li>
															      							<br>
															      							<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling 2014) </li>
															      							<br>
															      							<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br> (Goodfellow et al. 2014)</li>
															      							<br>
															      							<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017) </li>
															      						</ul>
															      					</div>
															      				</div>
															      				<br> <br> <br>
															      			</section>

															      			<section>
															      				<h3 class="slide-title"> A visual Turing test </h3>
															      				<div class="container">
															      					<div class="col">
															      						<img data-src="assets/samples_pixel_cnn.png" class="plain" style="height: 500px;"></img>
															      						<br>
															      						<div class="fragment fade-up" data-fragment-index="0"> Fake PixelCNN samples </div>
															      					</div>
															      					<div class="col">
															      						<img data-src="assets/sdss5.png" class="plain" style="height: 500px;"></img>
															      						<br>
															      						<div class="fragment fade-up" data-fragment-index="0"> Real galaxies from SDSS </div>
															      					</div>
															      				</div>
															      			</section>


<!--
            <section>

            				<section data-vertical-align-top>
            					<h3 class="slide-title">What Would a Bayesian Do?</h3>
            					$\boxed{y = \mathbf{A}x + n}$
            					<br>
            					<br>
            					The Bayesian view of the problem:
            					<br>
            					$$ p(x | y) \propto p(y | x) \ p(x) $$
            					<ul>
            						<li class="fragment fade-up">$p(y | x)$ is the data <b>likelihood</b>, which <b class="alert">contains the physics</b><br>
            						</li>
            						<br>
            						<li class="fragment fade-up">$p(x)$ is the <b>prior</b> knowledge on the solution.</li>
            					</ul>
            					<br>
            					<br>
            					<div class="fragment fade-up">
            						<ul>With these concepts in hand we can:
            							<br>
            							<li class="fragment">Estimate for instance the <b>Maximum A Posteriori</b> solution:
            								<br>
            								$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
            							</li>
            							<li class="fragment">Estimate from the <b>full posterior p(x|y)</b> with MCMC or Variational Inference methods.
            							</li>
            						</ul>
            					</div>
            					<br>
            					<div class="fragment fade-up">
            						<h3>How do you choose the prior ?</h3>
            					</div>
            				</section>

            				<section>
            					<h3 class="slide-title"> Classical examples of signal priors </h3>
            					<div class="container">
            						<div class="col">
            							Sparse
            							<img data-src="assets/wavelet.png" height="400" class="plain"></img><br>
            							$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
            						</div>
            						<div class="col">
            							Gaussian
            							<img data-src="assets/zknj8.jpg" height="400" class="plain"></img>
            							$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
            						</div>
            						<div class="col">
            							Total Variation
            							<img data-src="assets/shepp-Logan.ppm" class="plain"></img>
            							$$ \log p(x) = \parallel \nabla x \parallel_1 $$
            						</div>
            					</div>
            				</section>

          				<section data-background="assets/hsc_screen.png">
          					<h2>But what about this?</h2>
          				</section>
            </section> -->

			<!-- <section class="inverted" data-background="#000">
				<h2> Let's use <b>deep generative models</b> to build a tractable model of this high-dimensional prior distribution.</h2>
			</section> -->

  		<section>
  				<h3 class="slide-title">Getting started with Deep Priors: deep denoising example</h3>
  				$$ \boxed{{\color{Orchid} y}  = {\color{SkyBlue} x} + n} $$
  													<div class="container">
  														<div class="col">
  															<div style="position:relative; width:550px; height:550px; margin:0 auto;">
  																		<img class="fragment current-visible plain" data-src="assets/points.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
  																		<div class="fig-container fragment" data-file="dgm_prior_denoising.html" data-style="height: 550px;width: 550px;" style="position:absolute;top:0;left:0;" data-fragment-index="1"></div>
  															</div>
  															<!-- <img data-src="assets/points.png"/>
  															<div class="fig-container" data-file="dgm_prior_denoising.html" data-style="height: 550px;"></div> -->

  														</div>

  														<div class="col">
  															<ul>
  																<li class="fragment" data-fragment-index="0" > Let us assume we have access to examples of $ {\color{SkyBlue} x}$ without noise.</li>
  																<br>
  																<li class="fragment"  data-fragment-index="1">We learn the <b class="alert">distribution of noiseless data $\log p_\theta(x)$</b> from samples using a deep generative model.</li>
  																<br>
  																<!-- <li class="fragment"> We measure a noisy ${\color{Orchid} y}$ and we want to estimate a denoised ${\color{SkyBlue} x}$</li>
  																<br> -->
  																<li class="fragment">The solution should lie on the <b class="alert">realistic data manifold</b>, symbolized by the two-moons distribution.
  																	<div class="fragment">
  																	<p> We want to solve for the Maximum A Posterior solution: </p>
  																	$$\arg \max - \frac{1}{2} \parallel {\color{Orchid} y} - {\color{SkyBlue} x} \parallel_2^2 + \log p_\theta({\color{SkyBlue} x})$$

  																	This can be done by <b>gradient descent</b> as long as one has access to the <b class="alert">score function</b> $\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d \color{orange}x}$.
  																</div>
  																</li>
  															</ul>
  													</div>
  										</div>
  										</section>

			<!-- <section>
				<h2>Probabilistic Mapping of Dark Matter by<br> Neural Score Matching</h2>
				<a href="https://arxiv.org/abs/2011.08271"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2011.08271-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<hr>
				<div class="container">
					<div class="col">
						<div align="left" style="margin-left: 20px;">
							<h3>Work in collaboration with: <br>
								Benjamin Remy, Zaccharie Ramzi
							</h3>
							<img data-src="http://www.cosmostat.org/wp-content/uploads/2020/11/benjamin.png" style='width:200px; height:200px;'></img>
							<img data-src="http://www.cosmostat.org/wp-content/uploads/2019/03/Portrait-2-1600x2000.jpg" style='width:200px; height:200px;object-fit: cover;'></img>

							<br>

							$\Longrightarrow$ <b class="alert">Learn complex priors</b> by Neural Score Estimation and <b class="alert">sample from posterior</b> with gradient-based MCMC.
						</div>
					</div>
					<div class="col">
						<img class="plain" data-src="assets/knee.gif" style="width:450px;" />
					</div>
				</div>
				<br>
			</section> -->

			<section>
				<h3 class="slide-title">Writing down the convergence map log posterior</h3>
				<div class="container">
					<div class="col">
							<div style="float:right; font-size: 20px"> Remy, Lanusse, et al. (2022) <a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
					</div>
				</div>

					$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$

					<ul>
						<li> The likelihood term is <b class="alert">known analytically</b>.
						</li>

						<li class="fragment fade-up"> There is <b class="alert">no close form expression for the full non-Gaussian prior</b> of the convergence. However:
							<ul>
								<li class='fragment'> <b>We do have access to samples of full  <b class="alert">implicit</b> prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
									<img data-src='assets/plot_massive_nu.png' />
								</li>
							</ul>
						</li>
					</ul>
					<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior from simulation</b>,
						and then <b class="alert">sample the full posterior</b>.</div>
          </section>


		<section>
			<section>
				<h3 class="slide-title">The score is all you need!</h3>
				<br>

				<div class="container">
					<div class="col">
						<ul>
							<li> Whether you are looking for the MAP or sampling with HMC or MALA, you
								<b class="alert">only need access to the score</b> of the posterior:
								$$\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x \color{orange}|\color{orange} y\color{orange})}{\color{orange}
								d
								\color{orange}x}$$
								<ul>
									<li>Gradient descent: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) $</li>
									<li>Langevin algorithm: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) + \sqrt{2\tau} n_t$ </li>
								</ul>
							</li>
							<br>
						</ul>
					</div>
					<div class="col">
						<img data-src="assets/score_two_moons.png"></img>
					</div>
				</div>
				<br>
				<br>
				<!-- <ul>
					<li > The score of the full posterior is simply:
						$$\nabla_x \log p(x |y) = \underbrace{\nabla_x \log p(y |x)}_{\mbox{known}} \quad + \quad \underbrace{\nabla_x \log p(x)}_{\mbox{can be learned}}$$
						$\Longrightarrow$ all we have to do is <b class="alert">model/learn the score of the prior</b>.
					</li>
				</ul> -->
			</section>

				<section>
					<h3 class="slide-title">Neural Score Estimation by Denoising Score Matching</h3>

					<ul>
						<li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
							<ul>
								<li class="fragment fade-up"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
									$$x^\prime = x + u$$
								</li>
								<li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
									$$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
								</li>
								<li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
									$$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$$
								</li>
							</ul>
						</li>
					</ul>

					<div class="fragment fade-up">
						<div class="container">
							<div class="col">$\boldsymbol{x}'$
							</div>
							<div class="col">$\boldsymbol{x}$
							</div>
							<div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
							</div>
							<div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
							</div>
						</div>
						<img data-src="assets/denoised_mnist.png" style='width:1200px;'></img>
					</div>
				</section>

				<section>
					<h3 class="slide-title">Training a Neural Score Estimator in practice</h3>

					<div class="container">
						<div class="col">
							<br>
							<img data-src="assets/unet.png" data-fragment-index="1" /><br>
							<br> A standard UNet
						</div>

						<div class="col">
							<ul>
								<li> We use a very standard residual UNet, and we adopt a residual
									score matching loss:
									$$ \mathcal{L}_{DSM} = \underset{\boldsymbol{x} \sim P}{\mathbb{E}} \underset{\begin{subarray}{c}
									\boldsymbol{u} \sim \mathcal{N}(0, I) \\
									\sigma_s \sim \mathcal{N}(0, s^2)
									\end{subarray}}{\mathbb{E}} \parallel \boldsymbol{u} + \sigma_s \boldsymbol{r}_{\theta}(\boldsymbol{x} + \sigma_s \boldsymbol{u}, \sigma_s) \parallel_2^2$$
									$\Longrightarrow$ direct estimator of the score $\nabla \log p_\sigma(x)$
								</li>
								<br>
								<li class="fragment fade-up"> Lipschitz regularization to improve robustness:
									<br><br>
									<div class="container">
										<div class="col">
											Without regularization
										</div>

										<div class="col">
											With regularization
										</div>
									</div>
									<img data-src='assets/reg_score.png' />
								</li>
							</ul>
						</div>
					</div>
				</section>

				 <section>
								<h3 class="slide-title">Efficient sampling by Annealed HMC</h3>

								<ul>
									<li> Even with gradients, <b class="alert">sampling in high number of dimensions is difficult!</b><br>
										$\Longrightarrow$ Use a parallel annealing strategy to effectively sample from full distribution.
									</li>
									<br>
									<li class="fragment fade-up"> We use the fact that our score network $\mathbf{r}_\theta(x, \sigma)$ is learning a noise-convolved distribution $\nabla \log p_\sigma$

										<div>
											$$\sigma_1 > \sigma_2 > \sigma_3 > \sigma_4 $$
											<img data-src="assets/annealing.png" />
										</div>

									</li>

									<li class="fragment fade-up"> Run many HMC chains in parallel, progressively annealing the $\sigma$ to 0, <b class="chain">keep last point in the chain as independent sample</b>.
									</li>
								</ul>
							</section>

							<section>
									<h3 class="slide-title">Example of one chain during annealing</h3>
									<img data-src="assets/hmc-annealing.gif"/>
							</section>

			</section>

			<section>
				<section>
					<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>

					<div class="container">
						<div class="col">
							<img data-src='assets/ref_ktng.png' style="width:350px; height:350px;" />
							<br>
							True convergence map
						</div>
						<div class="col">
							<div class="block-content">
								<div style="position:relative; width:350px; height:350px; top:10px; left:40px;">
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="0">
										<img data-src='assets/ks_ktng.png' style="width:350px; height:350px;" />
									</div>
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="1">
										<img data-src='assets/wiener_ktng.png' style="width:350px; height:350px;" />
									</div>
									<div class="plain fragment" style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="2">
										<img data-src='assets/mean_post_ktng.png' style="width:350px; height:350px;" />
									</div>
								</div>
								<div class="block-content">
									<div style="position:relative; width:350px; height:20px; top:50px; left:10px;">
										<div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:350px;">Traditional Kaiser-Squires</div>
										<div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:350px;">Wiener Filter</div>
										<div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:350px;">Posterior Mean (ours)</div>
									</div>
								</div>
								<br>
								<br>
							</div>

						</div>
						<div class="col fragment">
							<img data-src='assets/cropped.gif' style="width:350px; height:350px;" />
							<br>
							Posterior samples
						</div>
					</div>

				</section>

				<section>
					<h3 class="slide-title">Validating Bayesian Posterior in Gaussian case</h3>

					<img  style="height:600px;" data-src="assets/Remy2022Wiener.png"/>

				</section>


			</section>


							<section>
								<h3 class="slide-title">Reconstruction of the <b class="alert">HST/ACS COSMOS field</b></h3>

								<ul>
								<li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
								</li>
								<li> Prior learned from $\kappa$-TNG simulation from <a href=https://arxiv.org/abs/2010.09731>Osato et al. 2021</a>.
								</li>
							</ul>
								<br>
								<div class="container">
									<div class="col">
										<div class="block-content">
											<div style="position:relative; height:570px; top:0px; left:0px;">
												Massey et al. (2007)
												<img data-src="assets/massey.png" style="height:500px;"></img>
											</div>
										</div>
									</div>

									<div class="col">
										<div class="block-content">
											<div style="position:relative; height:570px; top:0px; left:0px;">
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
													Remy et al. (2022) <b class="alert">Posterior mean</b>
													<img data-src='assets/remy.png' style="height:500px;" />
												</div>

												<div class="plain fragment" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
													Remy et al. (2022) <b class="alert">Posterior samples</b>
													<img data-src='assets/cosmos_samples.gif' style="height:500px;" />
												</div>

											</div>
										</div>
									</div>

								</div>
							</section>

      <section>
        <h3 class="slide-title">Other examples of Deep Priors</h3>
        <br>
        <div class="container">
          <div class="col">
            <ul>
              <li><em>Hybrid Physical-Deep Learning Model for Astronomical Inverse Problems</em><br> <b> F. Lanusse</b>, P. Melchior, F. Moolekamp<br>
                <a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
                <a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
              </li>
            </ul>
            <br> <br>
            $\mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i)$
            <br> <br>
            <img class=" plain" data-src="assets/scarlet_hsc.png" />
            <br> <br>

          </div>
          <div class="col fragment">
            <ul>
              <li><em>Denoising Score-Matching for Uncertainty Quantification in Inverse Problems</em><br> Z. Ramzi, B. Remy, <b>F. Lanusse</b>, P. Ciuciu, J.L. Starck<br>
                <a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
              </li>
            </ul>
            <img class="plain" data-src="assets/knee.gif" style="height:410px;"/>

          </div>
        </div>
      </section>
 <!--
			<section>
      <section data-background="assets/gal_hsc.png">
					<h3 class="slide-title">Example of application to deblending</h3>
					<br>
				 	<ul>
						<li><em>Hybrid Physical-Deep Learning Model for Astronomical Inverse Problems</em> <b> F. Lanusse</b>, P. Melchior, F. Moolekamp<br>
							<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
							<a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
						</li>
					</ul>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
      </section>


						<section>
							<h3 class="slide-title"> The Scarlet algorithm: deblending as an optimization problem</h3>
									<div style="float:right; font-size: 20px">Melchior et al. 2018</div>

									$$ \mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i) + \sum_{i=1}^K g_i(A_i) +  \sum_{i=1}^K f_i(S_i)$$

							<div class="container">
							<div class="col">
									<img data-src="assets/scarlet_data.png" height=450 class="plain"></img>
							</div>

							<div class="col">

								Where for a $K$ component blend:
								<br>
									<ul>
									<li>$P$ is the convolution with the instrumental response</li>
									<br>
									<li>$A_i$ are channel-wise galaxy SEDs, $S_i$ are the morphology models</li>
									<br>
									<li>$\mathbf{\Sigma}$ is the noise covariance</li>
									<br>
									<li>$\log p_\theta$ is a PixelCNN prior</li>
									<br>
									<li>$f_i$ and $g_i$ are arbitrary additional non-smooth consraints, e.g. positivity, monotonicity...</li>
									</ul>
							</div>
						</div>

						<span class="fragment fade-up">$\Longrightarrow$ Explicit physical modeling of the observed sky</span>
						</section>

						 <section>
							<h3  class="slide-title">Training the morphology prior</h3>

							<div class="container">
								<div class="col">
									<img data-src="assets/cosmos_training.png" height=450 class="plain"></img>
									<div> Postage stamps of isolated COSMOS galaxies used for training, at WFIRST resolution and fixed fiducial PSF</div>
							</div>

							<div class="col">
							<div class="container fragment fade-in">
								<div class="col">
									isolated galaxy
								<img data-src="assets/gal_1.png" class="plain"></img>
								<span> $\log p_\theta(x) = 3293.7$ </span>
							</div>

								<div class="col">
									artificial blend
								<img data-src="assets/gal_2.png" class="plain"></img>
								<span> $\log p_\theta(x) = 3100.5 $ </span>
							</div>
								</div>
							</div>
						</section>

						<section>
						<h3 class="slide-title">Scarlet in action</h3>

						<div class="container">
							<div class="col">
								Input blend
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
							<img data-src="assets/scar_input.png" class="plain"></img>
						</div>
							</div>

						<div class="col">
							<span class="fragment" data-fragment-index="0">Solution</span>
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
									  <img class="fragment current-visible plain" data-src="assets/old_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
									  <img class="fragment  plain" data-src="assets/pix_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
						</div>

						<div class="col">
							<span class="fragment" data-fragment-index="0">Residuals</span>
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
									  <img class="fragment current-visible plain" data-src="assets/old_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
									  <img class="fragment  plain" data-src="assets/pix_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
						</div>
						</div>

						<ul>
								<li class="fragment fade-up" data-fragment-index="0">Classic priors (monotonicity, symmetry).</li>
								<br>

								<li class="fragment fade-up" data-fragment-index="1">Deep Morphology prior.</li>
						</ul>

					</section>
					<section>
						<div class="container">
							<div class="col">
								True Galaxy
							<img data-src="assets/true_input.png" class="plain"></img>
						</div>

						<div class="col">
							Deep Morphology Prior Solution

										<img class=" plain" data-src="assets/pix_rec2.png"  />

						</div>

						<div class="col">
							Monotonicity + Symmetry Solution
										<img class=" plain" data-src="assets/scar_rec2.png" />
							</div>
						</div>
					</section>
		</section> -->
<!--
 									<section>
 										<h3 class="slide-title">Example of application to deblending</h3>
 										<br>
 										<div class="container">
 											<div class="col">
 												<ul>
 													<li><em>Hybrid Physical-Deep Learning Model for Astronomical Inverse Problems</em><br> <b> F. Lanusse</b>, P. Melchior, F. Moolekamp<br>
 														<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
 														<a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
 													</li>
 												</ul>
 												<br> <br>
 												$\mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i)$
 												<br> <br>
 												<img class=" plain" data-src="assets/scarlet_hsc.png" />
 												<br> <br>
 											</div>

 											<div class="col fragment fade-in">
 												<div class="container">
 													<div class="col">
 														isolated galaxy
 														<img data-src="assets/gal_1.png" class="plain"></img>
 														<span> $\log p_\theta(x) = 3293.7$ </span>
 													</div>

 													<div class="col">
 														artificial blend
 														<img data-src="assets/gal_2.png" class="plain"></img>
 														<span> $\log p_\theta(x) = 3100.5 $ </span>
 													</div>
 												</div>
 											</div>
 										</div>
 									</section> -->

									<!-- <section>
										<section>
											<h3 class="slide-title">Uncertainty quantification in Magnetic Resonance Imaging (MRI)</h3>
											<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
														src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
											</div>
											<br>
											<br>
											$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
											<div><video data-autoplay loop="loop" data-src="assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
											</div>
											<br>

											<br>

											<br>

											<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
										</section>
									</section> -->


      <!-- <section>
				<h3 class="slide-title">takeaways</h3>
				<br>
				<br>

				<div class="block ">
					<div class="block-title">
						Benefits of Bayesian forward modeling for inverse problems
					</div>
					<div class="block-content">
						Using a Bayesian approach has great advantages: <b>model-based physical interpretation & uncertainty quantification</b>.
						<br>
						<br>
						<ul>
							<li class="fragment"> <b class="alert">Explicit likelihood</b>, uses of all of our physical knowledge.<br>
								$\Longrightarrow$ The method can be applied for varying noise, observing conditions, or different instruments
							</li>
							<br>
							<li class="fragment"> Deep generative models can be used to provide <b class="alert">simulation or data driven priors</b>.<br>
								$\Longrightarrow$ Embed prior only accessible from samples (e.g. numerical simulations or data).
							</li>
						</ul>
					</div>
				</div>


				<br>
				<br>
				<br>
			</section> -->

			<section>
				<h2>Simulation-Based Inference <br> by Neural Summarisation and Density Estimation</h2>
				<a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://github.com/NiallJeffrey/Likelihood-free_DES_SV"><img src="https://badgen.net/badge/icon/github?icon=github&label" class="plain" style="height:25px;" /></a>
				<hr>
				<div class="container">
					<div class="col">
						<div align="left" style="margin-left: 20px;">
							<h3>Work in collaboration with Niall Jeffrey, Justin Alsing
							</h3>
							<img data-src="assets/niall.jpg" style='width:200px; height:200px;object-fit: cover;'></img>
							<img data-src="assets/justin.jpeg" style='width:200px; height:200px;'></img>

							<br>
							<br>
							$\Longrightarrow$ <b class="alert">Learn an implicit data likelihood</b> from simulations.
						</div>
					</div>
					<div class="col">
						<img class="plain" data-src="assets/ks_sv.png" style="width:350px;" />
					</div>
				</div>
				<br>
			</section>


      			<section>
      				<h3 class='slide-title'> limits of traditional cosmological inference </h3>
      				<div class='container'>
      					<div class='col'>
      						<div style="position:relative; width:480px; height:30px; margin:0 auto;">
      							<div class="fragment current-visible" style="position:absolute;top:0;" data-fragment-index="1">HSC cosmic shear power spectrum</div>
      							<div class="fragment" style="position:absolute;top:0;" data-fragment-index="2">HSC Y1 constraints on $(S_8, \Omega_m)$</div>
      						</div>
      						<div style="position:relative; width:480px; height:300px; margin:0 auto;">
      							<div class="fragment current-visible" style="position:absolute;top:0;left:0;" data-fragment-index="0">
      								<img class="plain" data-src="assets/alonso_g1.png" />
      								<img class="plain" data-src="assets/alonso_g2.png" />
      							</div>
      							<img class="fragment current-visible plain" data-src="assets/hsc_correlation_function.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
      							<img class="fragment  plain" data-src="assets/hsc_constraints.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
      						</div>
      						<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Hikage et al. 2018)</div>
      					</div>

      					<div class='col'>
      						<ul>
      							<li class="fragment" data-fragment-index="0"> Measure the ellipticity $\epsilon = \epsilon_i + \gamma$ of all galaxies<br>
      								$\Longrightarrow$ Noisy tracer of the weak lensing shear $\gamma$ </li>
      							<br>
      							<li class="fragment" data-fragment-index="1"> Compute <b class="alert">summary statistics</b> based on 2pt functions, <br>e.g. the <b>power spectrum</b> </li>
      							<br>
      							<li class="fragment" data-fragment-index="2"> Run an MCMC to recover a posterior on model parameters, using an <b class="alert">analytic likelihood</b>
      								$$ p(\theta | x ) \propto \underbrace{p(x | \theta)}_{\mathrm{likelihood}} \ \underbrace{p(\theta)}_{\mathrm{prior}}$$
      							</li>
      						</ul>
      					</div>
      				</div>

      				<div class="block fragment">
      					<div class="block-title">
      						Main limitation: the need for an explicit likelihood
      					</div>
      					<div class="block-content">
      						We can only compute the likelihood for <b class="alert">simple summary statistics</b> and on <b class="alert">large scales</b>
      						<br>
      						<br>
      						<div class="fragment"> $\Longrightarrow$ We are dismissing a large amount of information! </div>
      					</div>
      				</div>
      			</section>

            <section>
  						<h3 class='slide-title'>A different road: forward modeling</h3>

  						<div class='container'>
  							<div class='col'>
  								<ul>
  									<li> Instead of trying to analytically evaluate the likelihood,
  										let us build a forward model of the observables.</li>
  									<br>
  								</ul>
  								<br>
  								<br>
  								<br>
  								<div class="fragment">
  									$\Longrightarrow$ Learn an <b>implicit likelihood</b> $p(x|\theta)$ given by the <b class="alert">simulator as our physical model</b>
  								</div>
  								<br>
  								<br>
  								<br>
  								<br>
  							</div>

  							<div class='col'>

  								<div style="position:relative; width:600px; height:600px; margin:0 auto;">
  									<img class="plain" data-src="assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
  								</div>
  							</div>
  						</div>
  					</section>

      			<section>
      				<section>
      					<h3 class="slide-title">End-to-end framework for likelihood-free parameter inference with DES SV</h3>
      					<div class="container">
      						<div class="col">
      							<div style="float:right; font-size: 20px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
      										style="height:25px;vertical-align:middle;" /></a></div>
      						</div>
      					</div>

      					<div class="container">
      						<div class="col">
      							<img class="plain" data-src="assets/ks_sv.png" style="height:550px;"></img>
      						</div>

      						<div class="col fragment">
      							<img class="plain" data-src="assets/orthant.png" style="height:300px;" />
      							<img class="plain" data-src="assets/sim_params.png" style="height:300px;" /><br>
      							Suite of N-body + raytracing simulations: $\mathcal{D}$
      						</div>
      					</div>
      				</section>

							<section>
								<h3>Our method</h3>
								<div class="block fragment">
									<div class="block-title">
										A two-steps approach to inference
									</div>
									<div class="block-content">
										<ul>
											<li> Automatically learn an <b>optimal low-dimensional summary statistic</b>
												$$y = f_\varphi(\kappa_{KS}) $$
											</li>

											<br>

											<li class="fragment"> Use Neural Density Estimation to build an <b>estimate $p_\phi$ of the likelihood function $p(y \ | \ \theta)$</b> (Neural Likelihood Estimation)
											</li>

											<br>

											<li class="fragment"> Run a conventional Markov Chain Monte Carlo sampling
											</li>

												</ul>
											</li>
										</ul>
									</div>
								</div>
							</section>

						</section>

						<section>

      				<section>
      					<h3 class="slide-title">Learning summary statistics by Variational Mutual Information Maximization</h3>
      					<br>
      					<br>
      					<div class="container">

      						<div class="col">
      							<img class="plain" data-src="assets/mutual_information.png" />
      						</div>

      						<div class="col">
      							<ul>
      								<li> Mutual information between $X$ and $Y$:
      									<blockquote>
      										&ldquo;"amount of information" obtained about one random variable through observing the other random variable&rdquo;
      									</blockquote>
      								</li>
      								<br>
      								<li class="fragment">Given a parametric summarizing function $y = f_\phi(\kappa(\theta))$
      									<b class="alert">optimizing $f_\phi$ can be done by maximizing $I(y, \theta)$</b>.
      								</li>
      								<br>
      								<li class="fragment">In practice, $f_\phi$ is a CNN, trained to maximize a
      									variational lower bound on the mutual information:
      									$$ I(y ; \theta) \ \ge \ \mathbb{E}_{y, \theta} [ \log q_\phi(\theta | y) ] + H(\Theta) $$
      								</li>
      							</ul>
      						</div>
      					</div>
      				</section>

      				<section>
      					<h3 class='slide-title'> deep residual networks for lensing maps compression</h3>

      					<div class="container">

      						<div class="col" style="flex: 0 0 15em;">
      							<img class="plain" data-src="assets/jeffrey_model.png" style="height:550px" /><br>
      						</div>
      						<div class="col">
      							<ul>
      								<li> Deep Residual Network $y = f_\phi(x)$ followed by neural density estimator $q_\phi(\theta | y)$
      								</li>
      								<br>
      								<li class="fragment">Training on weak lensing maps simulated for different cosmologies</li>
      								<div class="container fragment">
      									<div class="col" style="flex: 0 0 26em;">
      										<img class="plain" data-src="assets/mass_maps.png" /><br>
      									</div>
      									<div class="col">
      										<img class="plain" data-src="assets/TF_FullColor_Horizontal.png" />
      										<br>
      										<br>
      										<br>
      										<img class="plain" data-src="assets/google-cloud-platform-logo.png" />
      									</div>
      								</div>
      								<li class="fragment">Training by Variational Mutual Information Maximization:
      									$$\mathbb{E}_{(x, \theta) \in \mathcal{D}} [ \log q_\phi(\theta | f_\phi(y) ) ]$$
      								</li>
      							</ul>
      						</div>
      					</div>
      				</section>
						</section>

						<section>

      				<section>
      					<h3 class="slide-title">Estimating the likelihood by Neural Density Estimation</h3>
      					<br>
      					$\Longrightarrow$ We cannot assume a Gaussian likelihood for the summary $y = f_\phi(\kappa)$ but we can learn $p(y | \theta)$: Neural Likelihood Estimation.
      					<br>
      					<br>
      					<div class="container">
      						<div class="col">
      							<img data-src="assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
      							<img data-src="assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="1"></img>
      							<br>
      							<div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
      						</div>
      						<div class="col">
      							<div class="block fragment fade-up" data-fragment-index="1">
      								<div class="block-title">
      									Neural Likelihood Estimation by Normalizing Flow
      								</div>
      								<div class="block-content">
      									<ul>
      										<li> We use a conditional Normalizing Flow to build an explicit model for the likelihood function
      											$$ \log p_\varphi (y | \theta)$$
      										</li>
      										<br>
      										<li class="fragment"> In practice we use the pyDELFI package and an <b>ensemble of NDEs</b> for robustness.
      										</li>
      										<br>
      										<li class="fragment"> Once learned, we can use the likelihood as part of a conventional MCMC chain</li>
      									</ul>
      								</div>
      							</div>
      							<br>
      							<br>
      						</div>
      					</div>
      				</section>

							</section>

      				<section>
      					<h3 class="slide-title">Parameter constraints from DES SV data</h3>

      					<div class="container">
      						<div class="col">
      							<img class="plain" data-src="assets/results_jeffrey.png" />
      						</div>

      						<div class="col fragment">
      							<img class="plain" data-src="assets/jeffrey_s8.png" />
      						</div>
      					</div>
      				</section>


					<section>
						<h2> GPU accelerated and Automatically Differentiable Physics </h2>
						<br>
						<a href="https://arxiv.org/abs/2108.13418"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2108.13418-B31B1B.svg" class="plain" style="height:25px;" /></a>
						<a href="https://arxiv.org/abs/2010.11847"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2010.11847-B31B1B.svg" class="plain" style="height:25px;" /></a>
						<a href="https://gitter.im/DifferentiableUniverseInitiative/community"><img src="https://badges.gitter.im/DifferentiableUniverseInitiative/jax_cosmo.svg" class="plain"
								style="height:25px;" /></a>
						<a href="https://github.com/DifferentiableUniverseInitiative"><img src="https://badgen.net/badge/icon/github?icon=github&label" class="plain" style="height:25px;" /></a>
						<hr>
						<br>
						<div class="container">
							<div class="col">
								<div align="left" style="margin-left: 20px;">
									<h3>Collection of collaborative projects, join us :-)</h3>
									<img style="height:375px;" data-src="assets/jax_cosmo_people_updated.png"/>
								</div>
							</div>
							<div>
								<img data-src="assets/flowpm.gif" />
							</div>
						</div>
						<br>
					</section>


      					<section>
      						<h3 class="slide-title">the hammer behind the Deep Learning revolution: Automatic Differentation</h3>

      						<ul>
      							<li class="fragment"> <b>Automatic differentiation</b> allows you to compute analytic derivatives of arbitraty expressions:<br>
      								If I form the expression $y = a * x + b$, it is separated in fundamental ops:
      								$$ y = u + b \qquad u = a * x $$
      								then gradients can be obtained by the chain rule:
      								$$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \frac{ \partial u}{\partial x} = 1 \times a = a$$
      							</li>
      							<br>
      							<li class="fragment"> This is a fundamental tool in Machine Learning, and autodiff frameworks include TensorFlow and PyTorch.
      							</li>
      						</ul>
      						<br>
      						<br>
      						<div class="block fragment">
      							<div class="block-title">
      								Enters JAX: NumPy + Autograd + GPU
      							</div>
      							<div class="block-content">

      								<div class="container">
      									<div class="col">
      										<ul>
      											<li>JAX follows the NumPy api!
      												<pre class="python"><code data-trim data-noescape>
      										import jax.numpy as np
      									</code></pre>
      											</li>
      											<li>Arbitrary order derivatives</li>
      											<li>Accelerated execution on GPU and TPU</li>

      										</ul>
      									</div>
      									<div class="col" align="center">

      										<img data-src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png" class="plain" />
      									</div>
      								</div>
      					</section>

      					<section>
      						<section>
      							<h3 class="slide-title"> jax-cosmo: Finally a differentiable cosmology library, and it's in JAX!</h3>

      							<div class="container">
      								<div class="col">
      									<img data-src="assets/github.png" class="plain" style="height:70px" />
      									<div> <a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo/">https://github.com/DifferentiableUniverseInitiative/jax_cosmo</a>
      									</div>

      									<pre class="python"><code data-trim data-noescape>
      										import jax.numpy as np
      										import jax_cosmo as jc

      										# Defining a Cosmology
      										cosmo = jc.Planck15()

      										# Define a redshift distribution with smail_nz(a, b, z0)
      										nz = jc.redshift.smail_nz(1., 2., 1.)

      										# Build a lensing tracer with a single redshift bin
      										probe = probes.WeakLensing([nz])

      										# Compute angular Cls for some ell
      										ell = np.logspace(0.1,3)
      										cls = angular_cl(cosmo_jax, ell, [probe])
      									</code></pre>

      									<div class="block">
      										<div class="block-title">
      											Current main features
      										</div>
      										<div class="block-content">
      											<ul>
      												<li>Weak Lensing and Number counts probes</li>
      												<li>Eisenstein & Hu (1998) power spectrum + halofit</li>
      												<li>Angular $C_\ell$ under Limber approximation </li>
      											</ul>
      											<div>$\Longrightarrow$ 3x2pt DES Y1 capable </div>
      										</div>
      									</div>

      								</div>

      								<div class="col">
      									<img class="plain" data-src="assets/jc_vs_ccl_lensing.png" />
      									<img class="plain" data-src="assets/jc_vs_ccl_clustering.png" />
      									<br>
      									Validating against the <a href="https://github.com/LSSTDESC/CCL">DESC Core Cosmology Library</a>
      								</div>
      							</div>
      						</section>

      						<section>
      							<h3 class="slide-title"> let's compute a Fisher matrix</h3>

      							<br>

      							$$F = - \mathbb{E}_{p(x | \theta)}[ H_\theta(\log p(x| \theta)) ] $$

      							<br>

      							<div class="container">
      								<div class="col fragment">

      									<pre class="python"><code data-trim data-noescape>
      					import jax
      					import jax.numpy as np
      					import jax_cosmo as jc

      					# .... define probes, and load a data vector

      					def gaussian_likelihood( theta ):
      					  # Build the cosmology for given parameters
      					  cosmo = jc.Planck15(Omega_c=theta[0], sigma8=theta[1])

      					  # Compute mean and covariance
      					  mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo,
      					                                                    ell, probes)
      					  # returns likelihood of data under model
      					  return jc.likelihood.gaussian_likelihood(data, mu, cov)

      					# Fisher matrix in just one line:
      					F = - jax.hessian(gaussian_likelihood)(theta)
      					</code></pre>
      									<a href="https://colab.research.google.com/github/DifferentiableUniverseInitiative/jax_cosmo/blob/master/docs/notebooks/jax-cosmo-intro.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg"
      											alt="Open In Colab" class="plain" style="height:25px;" /></a>
      								</div>

      								<div class="col fragment">
      									<img data-src="assets/Fisher_mat.png" class="plain"><br><br>
      								</div>
      							</div>

      							<ul>
      								<li class="fragment"> <b class="alert">No derivatives were harmed by finite differences in the computation of this Fisher!</b> </li>
      								<li class="fragment"> Only a small additional compute time compared to one forward evaluation of the model</li>
      							</ul>

      						</section>


									      					<section>
									      						<h3 class="slide-title"> Inference becomes fast and scalable</h3>

									      						<div class="container">
									      							<div class="col">

									      								<ul>
									      									<li>Current cosmological MCMC chains take <b>days</b>, and typically require access
									      										to large computer clusters.</li>
									      									<br>
									      									<li class="fragment" data-fragment-index="1"><b class="alert">Gradients of the log posterior are required for modern efficient and scalable inference</b> techniques:
									      										<ul>
									      											<li>Variational Inference</li>
									      											<li>Hamiltonian Monte-Carlo</li>
									      										</ul>
									      									</li>
									      									<br>
									      									<li class="fragment" data-fragment-index="2">In jax-cosmo, we can trivially obtain <b>exact</b> gradients:
									      										<pre class="python"><code data-trim data-noescape>
									      					def log_posterior( theta ):
									      					    return gaussian_likelihood( theta ) + log_prior(theta)

									      					score = jax.grad(log_posterior)(theta)
									      					</code></pre>
									      									</li>

									      									<br>
									      									<li class="fragment" data-fragment-index="3">On a DES Y1 analysis, we find convergence in 70,000 samples with vanilla HMC, 140,000 with Metropolis-Hastings</li>
									      								</ul>

									      							</div>

									      							<div class="col">
									      								<div class="fragment" data-fragment-index="3">
									      									<img data-src="assets/jc_3x2pt_hmc.png" class="plain" /><br>
									      									DES Y1 posterior, jax-cosmo HMC vs Cobaya MH <br>(credit: Joe Zuntz)
									      								</div>
									      							</div>
									      						</div>
									      					</section>
      					</section>


      					<section>
									<section>
										<h3 class="slide-title">LSST DESC 3x2pt Tomography Challenge</h3>
										<div class="container">
											<div class="col">
												<div style="float:right; font-size: 20px"> Zuntz, <b>Lanusse</b>, et al. (2021)
													<a href="https://arxiv.org/abs/2108.13418"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2008.13418-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
												</div>
											</div>
										</div>
										<div class="block">
											<div class="block-title">
												Description of the challenge
											</div>
											<div class="block-content">
												<blockquote>
													&ldquo;Given (g)riz photometry, find a tomographic bin assignment
													method that optimizes a 3x2pt analysis.&rdquo;
												</blockquote>
												<ul>
													<li> Metrics: Total Signal-to-Noise: $m_{SNR} = \sqrt{\mu^t \mathbf{C}^{-1} \mu}$ ; <b class="alert">DETF Figure of Merit</b>: $m_{FOM} = \frac{1}{\sqrt{ \det(\mathbf{F}^{-1})}}$
													</li>
													<li> Idealized setting: assumes perfect training set. More info at: <a href="https://github.com/LSSTDESC/tomo_challenge">https://github.com/LSSTDESC/tomo_challenge</a>
													</li>
												</ul>
											</div>
										</div>

										<div class="container">
											<div class="col">
												<img class="plain fragment" data-src="assets/nnexample.png" data-fragment-index="1" />
											</div>

											<div class="col">
												<ul>
													<li class="fragment" data-fragment-index="0"> <b>Conventional strategy</b>: Use a photoz code to estimate redshifts,
														then bin galaxies based on their photoz.
													</li>
													<br>
													<li class="fragment" data-fragment-index="1"> <b class="alert">Strategy with Differentiable Physics</b>:
														<ul>
															<li> Introduce a parametric bin assignement function $f_\theta(x_{phot})$</li>
															<li> Optimize $\theta$ by back-propagating through the challenge metrics. </li>
													</li>

												</ul>
											</div>
										</div>
									</section>

      						<section>
      						<iframe width="100%" height="849" frameborder="0"
      						  src="https://observablehq.com/embed/@eiffl/tomo-challenge-results-visualization?cells=viewof+results_bands%2Cmain_plot"></iframe>
      						</section>
      					</section>

									<section>
      					<section>
      							<h3 class='slide-title'>introducing FlowPM: Particle-Mesh Simulations in TensorFlow</h3>
      							<div class="container">
      								<div class="col">
      									<div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak (2020)
      										<a href="https://arxiv.org/abs/2010.11847"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2010.11847-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
      								</div>
      							</div>
      							<div class='container'>
      								<div class='col'>
      									<img data-src="assets/github.png" class="plain" style="height:70px" />
      									<img data-src="assets/TF_FullColor_Horizontal.png" class='plain' style="height: 70px;" />

      									<div> <a href="https://github.com/DifferentiableUniverseInitiative/flowpm">https://github.com/DifferentiableUniverseInitiative/flowpm</a>
      									</div>
      									<pre class="python"><code data-trim data-noescape>
      													import tensorflow as tf
      													import flowpm
      													# Defines integration steps
      													stages = np.linspace(0.1, 1.0, 10, endpoint=True)

      													initial_conds = flowpm.linear_field(32,       # size of the cube
      													                                   100,       # Physical size
      													                                   ipklin,    # Initial powerspectrum
      													                                   batch_size=16)

      													# Sample particles and displace them by LPT
      													state = flowpm.lpt_init(initial_conds, a0=0.1)

      													# Evolve particles down to z=0
      													final_state = flowpm.nbody(state, stages, 32)

      													# Retrieve final density field
      													final_field = flowpm.cic_paint(tf.zeros_like(initial_conditions),
      													                               final_state[0])
      												</code></pre>
      									<ul>
      										<li> Seamless interfacing with deep learning components
      										</li>
      										<li> <b class="alert">Mesh TensorFlow</b> implementation for distribution on supercomputers
      										</li>
      									</ul>
      									<br>
      									<br>
      									<br>
      									<br>
      									<br>
      								</div>

      								<div class='col'>
												<img data-src="assets/flowpm.gif"></img>
      									<!-- <div class="fig-container" data-file="flowpm_16.html" data-style="height: 550px;"></div> -->
      									<br>
      									<br>
      									<br>
      									<br>
      								</div>
      							</div>
      						</section>

									<section>
										<h3 class='slide-title'>Mesh FlowPM: distributed, GPU-accelerated, and automatically differentiable simulations</h3>
										<!--
							<img data-src="assets/mesh_flopwm.png" class="plain" style="height:450px;" /> -->

										<div class="container">
											<div class="col">
												<img data-src="assets/mfpm_demo_1024.png" />
											</div>

											<div class="col">
												<ul>
													<li> We developed a <b class="alert">Mesh TensorFlow</b> implementation that can scale on GPU clusters (horovod+NCCL).
													</li>
													<br>
													<br>
													<li> For a $2048^3$ simulation:
														<ul>
															<li>Distributed on <b>256</b> NVIDIA V100 GPUs</li>
															<li>Runtime: 3 mins</li>
														</ul>
													</li>
													<br>
													<br>
													<li> Don't hesitate to reach out if you have a use case for model parallelism!<br>
														<img data-src="assets/github.png" class="plain" style="height:70px" /><br>

														<div> <a href="https://github.com/DifferentiableUniverseInitiative/mesh">https://github.com/DifferentiableUniverseInitiative/mesh</a>
														</div>
													</li>
												</ul>
											</div>
										</div>
									</section>
									</section>

      						<!-- <section>
      							<section>
      								<h3 class='slide-title'>Example use-case: reconstructing initial conditions by MAP optimization</h3>
      								<img data-src="assets/evolvingLSS.jpg" class="plain" /><br>
      								<div class="fragment">Going back to simpler times...</div>

      								<div class="fragment">
      									$$\arg\max_z \ \log p(x_{dm} = f(z)) \ + \ p(z) $$
      									where:<br>
      									<ul>
      										<li> $f$ is <b>FlowPM </b>
      										</li>
      										<li> $z$ are the initial conditions (early universe)
      										</li>
      										<li> $x_{dm}$ is the present day dark matter distribution
      										</li>
      									</ul>
      								</div>
      							</section> -->
<!--
      							<section>
      								<h3 class="slide-title"> MAP optimization in action</h3>
      								$$\arg\max_z \ \log p(x_{dm} = f(z)) \ + \ p(z) $$
      								<div style="float:right; font-size: 16px">credit: <a href="https://github.com/modichirag">C. Modi</a></div>
      								<br>
      								<div class="container">
      									<div class="col fragment fade-up">
      										<img data-src="assets/init_field.png" style='height:250px;' />
      										<br> True initial conditions <br> $z_0$
      									</div>

      									<div class="col">
      										<img data-src="assets/reconim_init.gif" style='height:250px;' />
      										<br> Reconstructed initial conditions $z$
      									</div>

      									<div class="col">
      										<img data-src="assets/reconim_fin.gif" style='height:250px;' />
      										<br> Reconstructed dark matter distribution $x = f(z)$
      									</div>

      									<div class="col">
      										<img data-src="assets/fin_field.png" style='height:250px;' />
      										<br> Data <br> $x_{DM} = f(z_0)$
      									</div>
      								</div>
      								<br>
      								<br>

      								<div class="fragment">
      									Check out this blogpost for more details <br> <a href=https://blog.tensorflow.org/2020/03/simulating-universe-in-tensorflow.html>
      										https://blog.tensorflow.org/2020/03/simulating-universe-in-tensorflow.html</a>
      								</div>
      							</section>
      						</section>

									<section>
										<section>
											<h3 class="slide-title">CosmicRIM: Recurrent Inference Machines for Initial Condition Reconstruction</h3>
											<div class="container">
												<div class="col">
													<div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak, Spergel, Perreault-Levasseur (2021)
														<a href="https://arxiv.org/abs/2104.12864"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2104.12864-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
													</div>
												</div>
											</div>
											<div class="container">

												<div class="col">
													Recurrent Neural Network Architecture
													<img data-src="assets/cosmic_rim.png" width="450" />
												</div>

												<div class="col fragment">
														Initial conditions cross-correlation
														<img data-src="assets/cosmic_rim_rc.png" width="500" />
												</div>
											</div>
											<ul>
												<li class="fragment">CosmicRIM: Learn to optimize by embedding a Neural Network in the optimization algorithm.<br>
													$\Longrightarrow$ converges 40x faster than LBFGS.</li>
											</ul>
										</section> -->

										<!-- <section>
											<h3 class="slide-title">Experiments</h3>
											<div class="block ">
												<div class="block-title">
													Settings
												</div>
												<div class="block-content">
													<ul>
														<li>Forward model: $64^3$ particles, 400 Mpc/h box, 2LPT dynamics with 2nd order bias model
														</li>
														<li> RIM: 10 steps, trained under l2 loss
														</li>
													</ul>
												</div>
											</div>

											<div class="container">

												<div class="col">
													Initial conditions cross-correlation
													<img data-src="assets/cosmic_rim_rc.png" width="500" />

												</div>
												<div class="col">
													Transfer function<br>
													<img data-src="assets/rim_transfer.png" width="500" />
												</div>

											</div>
											<ul>
												<li>CosmicRIM: Learn to optimize by embedding a Neural Network in the optimization algorithm.<br>
													$\Longrightarrow$ converges 40x faster than LBFGS.</li>
											</ul>

										</section> -->
									<!-- </section> -->

									<section>
										<h1> Conclusion </h1>
									</section>

									<section>
										<h3 class="slide-title"> Conclusion </h3>
										<div class="block ">
											<div class="block-title">
												Merging Deep Learning with Physical Models for Bayesian Inference
											</div>
											<div class="block-content">
												$\Longrightarrow$ Makes <b>Bayesian inference possible</b> at scale and with non-trivial models!
												<br>
												<br>
												<ul>


												<li class="fragment"> Complement known physical models with data-driven components
													<ul>
														<li>Use data-driven generative model as prior for solving inverse problems.</il>
													</ul>
													</il>
													<br>

													<li class="fragment"> Enables inference in high dimension from numerical simulators.
														<ul>
															<li>Automagically construct summary statistics.</li>
															<li>Provides the density estimation tools needed.</li>
														</ul>
														</il>
														<br>



													<li class="fragment"> Differentiable physical models for fast inference
														<ul>
															<li> Differentiability enables Bayesian inference over large scale simulations.</li>
															<li> Models can directly be embedded alongside deep learning components.</li>
														</ul>
													</li>
													<br>
												</ul>
											</div>
										</div>
										<br>

										<br>
										<p class="fragment">Thank you ! </p>
										<br> <br> <br>
									</section>

		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>
